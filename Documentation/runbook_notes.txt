================================================================================
ARCHITECTURAL STYLE CLASSIFICATION - COMPLETE RUNBOOK
================================================================================

This runbook provides step-by-step instructions for preparing data, training
models, and running inference for the architectural style classification
project using PyTorch.

================================================================================
SECTION 1: DATA PREPARATION
================================================================================

1.1 UNIFIED DATA PREPARATION PIPELINE
--------------------------------------
Script: house_classification/prepare_data.py

Purpose: Unified pipeline that handles image scraping, cleanup, and splitting
into train/validation/test sets.

Recommended Usage (All steps):
    cd house_classification
    python prepare_data.py --all

Individual Steps:
    python prepare_data.py --scrape   # 1. Download images via SerpAPI
    python prepare_data.py --cleanup  # 2. Remove corrupted/duplicates
    python prepare_data.py --split    # 3. Train/val/test split

Output Structure:
    architectural_style_images/
    ├── train/
    │   ├── american_craftsman/
    │   ├── colonial/
    │   └── ...
    ├── validation/
    │   └── ...
    └── test/
        └── ...

Note: Images are copied (not moved) during split, preserving original dataset.

1.2 LEGACY DATA SCRIPTS
------------------------
Individual data preparation scripts can still be run directly:
    python -m data_prep.scraper   # Image scraping
    python -m data_prep.cleanup   # Duplicate/corruption removal
    python -m data_prep.splitter  # Train/val/test split


================================================================================
SECTION 2: MODEL TRAINING
================================================================================

2.1 CONFIGURATION
-----------------
Configuration File: conf/training_config.yaml

Edit conf/training_config.yaml to set default parameters for:
    - Data paths (train/validation/test directories)
    - Batch size, number of workers, image size
    - Data augmentation settings (including CutMix)
    - Training hyperparameters (epochs, learning rate, etc.)
    - Normalization statistics
    - Model settings (freeze_features, dropout_rate)
    - MLflow experiment tracking

All training scripts use these defaults, which can be overridden via CLI.


2.2 DATA AUGMENTATION (INCLUDING CUTMIX)
-----------------------------------------
The pipeline includes comprehensive augmentation:
    - Random crop and horizontal flip
    - Rotation and color jitter
    - CutMix (image-mixing technique)

CutMix Configuration (conf/training_config.yaml):
    augmentation:
      cutmix:
        enabled: true   # Enable CutMix augmentation
        alpha: 1.0      # Beta distribution parameter (higher = more mixing)

CutMix:
    - Cuts a random rectangular region from one image and pastes it on another
    - Mixes labels proportionally to the area of the patch
    - Highly effective at smoothing decision boundaries
    - Reduces overfitting and improves generalization
    - Uses PyTorch's built-in torchvision.transforms.v2.CutMix

Note: CutMix requires torchvision >= 0.15.0


2.3 TRAINING MODELS
-------------------
Script: house_classification/train.py

Interactive Mode (Recommended):
    cd house_classification
    python train.py

    The script will prompt you to:
    1. Select model architecture (Vanilla CNN, ResNet-18, etc.)
    2. Optionally enable MLflow tracking

Command-Line Mode:
    python train.py --model resnet18 --epochs 30 --batch_size 32 --lr 0.001 --mlflow

Available Models:
    - vanilla         : Custom baseline CNN
    - resnet18        : Pretrained ResNet-18 (recommended)
    - resnet34        : Pretrained ResNet-34
    - resnet50        : Pretrained ResNet-50
    - efficientnet_b0 : Pretrained EfficientNet-B0
    - mobilenet_v2    : Pretrained MobileNet-V2 (lightweight)

Parameters:
    --model           : Model architecture
    --epochs          : Maximum training epochs (default: from config)
    --batch_size      : Images per batch (default: from config)
    --lr              : Learning rate (default: from config)
    --patience        : Early stopping patience (default: from config)
    --freeze_features : Freeze pretrained layers (only train classifier head)
    --resume          : Path to checkpoint to resume training
    --mlflow          : Enable MLflow experiment tracking
    --no_mlflow       : Disable MLflow tracking

Output Structure:
    checkpoints/<model>_ep<epochs>_bs<batch_size>_lr<lr>/
        ├── best_model.pth            # Best model (lowest val loss)
        ├── final_model.pth           # Final model (last epoch)
        ├── training_history.json     # Loss and accuracy per epoch
        ├── training_diagnostics.txt  # Training configuration and runtime
        └── image_batches/            # Sample augmented images per class
            ├── american_craftsman.png
            ├── colonial.png
            └── ...

Note: CutMix augmented samples are visualized in image_batches/ folder.


2.4 HYPERPARAMETER TUNING
--------------------------
Script: house_classification/tune_hyperparameters.py
Configuration: conf/tuning_config.yaml

Purpose: Systematically search for optimal hyperparameters using grid search,
Bayesian optimization, or genetic algorithms.

Search Methods:
    - grid_search : Exhaustive search over all combinations
    - bayesian    : Gaussian Process optimization
    - genetic     : Evolutionary algorithm

Usage:
    cd house_classification
    python tune_hyperparameters.py

    Prompts:
    1. Select model architecture
    2. Enable MLflow tracking? (Yes/No)

CLI Options:
    --config    Path to tuning config YAML
    --quick     Use reduced grid for faster testing
    --no_plots  Skip visualization generation
    --mlflow    Enable MLflow tracking

Output Directory:
    checkpoints_tuning/<model>/<search_method>/
        ├── trials/                        # Individual trial checkpoints
        ├── tuning_results.json            # All trial results
        ├── diagnostics_summary.txt        # Summary and top configurations
        └── *.png                          # Heatmaps and visualizations


================================================================================
SECTION 3: UNIFIED INFERENCE PIPELINE
================================================================================

3.1 OVERVIEW
------------
Script: house_classification/inference.py
Configuration: conf/inference.yaml

The unified inference pipeline consolidates all inference functionality into
a single, configuration-driven tool with six operational modes.

Modes:
    - evaluate                   : Full model evaluation with metrics
    - explain_single             : Grad-CAM for one image
    - explain_batch              : Grad-CAM for multiple images
    - explain_misclassifications : Analyze model errors
    - inference                  : Flexible inference with custom outputs
    - production                 : Complete pipeline with quality checks

Documentation: See Documentation/INFERENCE.md for complete guide


3.2 EVALUATE MODE
-----------------
Purpose: Evaluate model on test set with comprehensive metrics and visualizations.

Basic Usage:
    cd house_classification
    python inference.py
    # Uses default config: ../conf/inference.yaml (mode: evaluate)

Configuration (conf/inference.yaml):
    mode: "evaluate"

    model:
      checkpoint: "../checkpoints/resnet18_ep50_bs32_lr0.001/best_model.pth"

    evaluation:
      metrics:
        accuracy: true
        per_class_metrics: true
        confusion_matrix: true
        top_k_accuracy: true

Output Files (saved to checkpoint_dir/results/):
    - evaluation_results.json         # All metrics in JSON
    - evaluation_summary.txt          # Human-readable summary
    - confusion_matrix.png            # Confusion matrix heatmap
    - class_accuracy.png              # Per-class accuracy
    - comprehensive_analysis.png      # 6-panel visualization
    - per_class_metrics.png           # Precision/recall/F1 bars
    - train_history.png               # Training curves
    - classification_report.txt       # sklearn report


3.3 GRAD-CAM EXPLAINABILITY MODES
----------------------------------

3.3.1 Single Image Explanation
    Mode: explain_single

    Configuration:
        mode: "explain_single"
        paths:
          image_path: "architectural_style_images/test/gothic/image_001.jpg"
        explain_single_mode:
          true_class: null  # Optional ground truth
        gradcam:
          enabled: true
          top_k: 5

    Output:
        - 4-panel Grad-CAM visualization (original, heatmap, overlay, top-K)
        - Class comparison showing attention across top predictions

3.3.2 Explain Misclassifications
    Mode: explain_misclassifications

    Configuration:
        mode: "explain_misclassifications"
        explain_misclassifications_mode:
          limit: 20               # Max misclassifications to analyze
          sort_by: "confidence"   # "confidence" or "random"

    Prerequisites: Run evaluate mode first to generate evaluation results

    Output:
        - Grad-CAM visualizations for each misclassified image
        - Shows true label vs predicted label
        - Helps debug model errors

3.3.3 Batch Processing
    Mode: explain_batch

    Configuration:
        mode: "explain_batch"
        paths:
          image_dir: "architectural_style_images/test"
        processing:
          limit: null  # null = all images

    Output:
        - Grad-CAM for all images in directory
        - Organized output with sequential naming


3.4 PRODUCTION MODE WITH QUALITY CHECKS
----------------------------------------
Mode: production

Purpose: Complete inference pipeline with quality metrics and suspicious
prediction flagging.

Configuration:
    mode: "production"

    production_mode:
      run_evaluation: false
      run_quality_checks: true
      generate_report: true

    quality_metrics:
      enabled: true
      flag_suspicious: true
      thresholds:
        min_confidence_drop: 10.0      # Flag if drop < 10%
        min_concentration: 0.25         # Flag if concentration < 0.25
        max_coverage: 0.75              # Flag if coverage > 75%
        min_coverage: 0.05              # Flag if coverage < 5%

    outputs:
      gradcam_heatmaps:
        save_suspicious_only: true     # Only save flagged predictions
        separate_dirs: true             # normal/ and suspicious/ subdirs

Quality Metrics:
    - Confidence Drop: How much confidence decreases when model sees only
      the heatmap region (should be significant if attention is meaningful)
    - Confidence Increase: How much confidence changes when hiding the
      heatmap region
    - Concentration: How focused the heatmap is (0-1, higher = more focused)
    - Coverage: Percentage of image the heatmap covers

Output Structure:
    checkpoints/resnet18_ep50_bs32_lr0.001/results/
    ├── inference_metadata.jsonl    # Compact metadata (25-50x smaller)
    ├── inference_report.txt         # Summary report
    ├── inference.log                # Processing log
    ├── heatmaps/                    # Normal predictions
    │   └── *.png
    └── suspicious/                  # Flagged predictions
        └── *_FLAGGED.png


3.5 CLI OVERRIDES
-----------------
Override config values via command line:

    # Override mode
    python inference.py --mode production

    # Override paths
    python inference.py --image_dir data/new_images --output_dir results/

    # Override checkpoint
    python inference.py --checkpoint checkpoints/efficientnet_b0.../best_model.pth

    # Use custom config
    python inference.py --config conf/my_config.yaml


3.6 METADATA STORAGE
--------------------
Efficient JSONL format stores metadata without full heatmap images.

Query Metadata:
    from house_classification.explainability import MetadataStorage

    # Load metadata
    metadata = MetadataStorage.load("results/inference_metadata.jsonl")

    # Find suspicious predictions
    suspicious = MetadataStorage.query_suspicious(metadata)

    # Low confidence predictions
    low_conf = MetadataStorage.query_low_confidence(metadata, threshold=0.7)

    # Specific class
    gothic = MetadataStorage.query_by_class(metadata, "Gothic")


3.7 STREAMLIT WEB APP
---------------------
Script: house_classification/app_ml_engineer.py

Purpose: Interactive web app for single-image inference with Grad-CAM.

Usage:
    cd house_classification
    streamlit run app_ml_engineer.py
    # Open http://localhost:8501

Features:
    - Upload images via drag-and-drop
    - Instant Grad-CAM explanations
    - Top-K predictions with confidence scores
    - Interactive visualization


================================================================================
SECTION 4: MLFLOW EXPERIMENT TRACKING
================================================================================

4.1 OVERVIEW
------------
MLflow is integrated across the entire pipeline for experiment tracking.

Benefits:
    - Track all experiments in one place
    - Compare models and hyperparameters
    - Reproducible experiments
    - Visualize training curves
    - Store artifacts and visualizations
    - Web UI for browsing runs

Configuration (conf/training_config.yaml):
    mlflow:
      enabled: true                              # Enable by default
      tracking_uri: null                         # null = local ./mlruns
      experiment_name: "architectural-style-training"


4.2 THREE-TIER CONFIGURATION
-----------------------------
All scripts (train.py, tune_hyperparameters.py, inference.py) use:

1. CLI flags (highest priority)
   --mlflow or --no_mlflow

2. Interactive prompt (middle)
   User is prompted if config enabled and no CLI flag

3. Config file (lowest priority)
   conf/training_config.yaml or conf/inference.yaml


4.3 MLFLOW EXPERIMENTS
----------------------
Separate experiments for different pipeline stages:
    - architectural-style-training      : Training runs
    - architectural-style-tuning        : Hyperparameter tuning
    - architectural-style-evaluation    : Evaluation runs
    - architectural-style-explainability: Grad-CAM analyses


4.4 VIEWING RESULTS
-------------------
Start MLflow UI:
    cd /Users/tawate/Documents/Architecture_Image_Modeling
    mlflow ui
    # Open http://localhost:5000

Navigate:
    1. Select experiment from sidebar
    2. Browse runs (sort by metrics, filter by parameters)
    3. Click run to see details (parameters, metrics, artifacts)
    4. Compare multiple runs side-by-side

Documentation: See Documentation/MLFLOW_COMPLETE_GUIDE.md


4.5 SYSTEM METRICS MONITORING
------------------------------
When MLflow is enabled during training, system resource usage is automatically
tracked in the background.

Monitored Metrics:
    - CPU: utilization %, core count
    - Memory: RAM usage (GB and %), swap usage
    - Disk I/O: read/write throughput (MB), operation counts
    - Network I/O: sent/received data (MB), packet counts
    - GPU (if available): utilization %, memory (GB), temperature, power

Implementation:
    - house_classification/utils/system_metrics.py
    - Metrics collected every 5 seconds in background thread
    - Summary statistics (avg/max) logged to MLflow

Dependencies:
    pip install psutil nvidia-ml-py

View in MLflow UI:
    - system/avg_cpu_percent, system/max_cpu_percent
    - system/avg_memory_used_gb, system/max_memory_used_gb
    - system/avg_gpu_0_utilization_percent


================================================================================
SECTION 5: MODEL ARCHITECTURES
================================================================================

5.1 AVAILABLE MODELS
--------------------
VanillaCNN (Baseline):
    - Custom 4-layer CNN built from scratch
    - Good for baseline comparison

Pretrained Models (Transfer Learning):
    - ResNet: 18, 34, 50 (good balance of speed/accuracy)
    - EfficientNet: B0 (efficient architecture)
    - MobileNet: V2 (lightweight for deployment)

All pretrained models support:
    - Fine-tuning (train all layers)
    - Frozen features (train only classifier head)


5.2 GRAD-CAM SUPPORT
--------------------
Automatic target layer selection for all architectures.

Supported:
    - VanillaCNN
    - ResNet (18, 34, 50, 101, 152)
    - EfficientNet (B0-B4)
    - MobileNet (V2, V3)
    - VGG (11, 13, 16, 19)

Output:
    - 4-panel visualizations
    - Class comparison across top predictions
    - Misclassification analysis

Documentation: See Documentation/GRADCAM.md


================================================================================
SECTION 6: COMPLETE WORKFLOW
================================================================================

6.1 RECOMMENDED PIPELINE
-------------------------

Step 1: Data Preparation
    cd house_classification
    python prepare_data.py --all

Step 2: Configure Training
    Edit conf/training_config.yaml:
        - Set data paths
        - Enable CutMix if desired
        - Configure MLflow

Step 3: Train Models
    # Baseline
    python train.py --model vanilla --epochs 50 --mlflow

    # Transfer learning
    python train.py --model resnet18 --epochs 30 --mlflow
    python train.py --model efficientnet_b0 --epochs 30 --mlflow

Step 4: Evaluate Models
    Edit conf/inference.yaml:
        mode: "evaluate"
        model:
          checkpoint: "../checkpoints/resnet18_ep30_bs32_lr0.001/best_model.pth"

    python inference.py --mlflow

Step 5: Analyze Misclassifications
    Edit conf/inference.yaml:
        mode: "explain_misclassifications"
        explain_misclassifications_mode:
          limit: 20

    python inference.py --mlflow

Step 6: Production Inference
    Edit conf/inference.yaml:
        mode: "production"
        quality_metrics:
          enabled: true
          flag_suspicious: true

    python inference.py --mlflow

Step 7: Review in MLflow UI
    mlflow ui
    # Compare all experiments
    # Identify best model
    # Analyze quality metrics


6.2 HYPERPARAMETER TUNING WORKFLOW
-----------------------------------
Step 1: Configure Tuning
    Edit conf/tuning_config.yaml:
        - Set search_method (grid_search, bayesian, genetic)
        - Define search space

Step 2: Run Tuning
    python tune_hyperparameters.py --mlflow

Step 3: View Results
    mlflow ui
    # Find best hyperparameters

Step 4: Train Final Model
    python train.py --model resnet18 \
        --lr 0.001 --batch_size 32 --epochs 50 --mlflow


================================================================================
SECTION 7: TROUBLESHOOTING
================================================================================

7.1 TRAINING ISSUES
-------------------
Out of Memory (OOM):
    - Reduce --batch_size (try 16 or 8)
    - Disable CutMix temporarily (set enabled: false in config)

Slow Training:
    - Reduce num_workers in conf/training_config.yaml
    - Use smaller model (mobilenet_v2 instead of resnet50)
    - Reduce image size in config

Model Not Improving:
    - Check data split completed successfully
    - Try different learning rate
    - Increase patience for early stopping
    - Check training_diagnostics.txt


7.2 INFERENCE ISSUES
--------------------
No Images Found:
    - Check paths in conf/inference.yaml
    - Verify image extensions in processing config

Checkpoint Not Found:
    - Use absolute path or correct relative path in config
    - Verify checkpoint file exists

Too Many Flagged Predictions:
    - Adjust quality_metrics thresholds to be more lenient
    - Reduce min_confidence_drop or min_concentration


7.3 MLFLOW ISSUES
-----------------
MLflow UI Shows No Experiments:
    - Run "mlflow ui" from project root where mlruns/ exists

Runs Not Showing Up:
    - Refresh the page or restart mlflow ui
    - Check training completed successfully

Artifacts Not Loading:
    - Check file permissions in mlruns/ directory


================================================================================
SECTION 8: TIPS AND BEST PRACTICES
================================================================================

1. Always use random seed for reproducibility (especially data splits)

2. Start with vanilla CNN baseline, then transfer learning

3. Use interactive mode for exploration, CLI mode for automation

4. Enable CutMix for better generalization (requires torchvision >= 0.15.0)

5. Monitor validation accuracy during training with tqdm progress bars

6. Use evaluate mode to generate comprehensive visualizations

7. Use explain_misclassifications mode to debug model errors

8. Enable quality metrics in production mode to flag suspicious predictions

9. Use MLflow to track all experiments for reproducibility

10. Review metadata JSONL files for efficient analysis (25-50x smaller than images)

11. For production, use MobileNet-V2 or EfficientNet-B0 for faster inference

12. Check class balance - consider weighted loss if imbalanced

13. Use --freeze_features with pretrained models when you have limited data

14. Archive important evaluation results before rerunning

15. Back up mlruns/ directory for experiment history


================================================================================
SECTION 9: CODE ORGANIZATION
================================================================================

Project Structure:
    house_classification/
    ├── prepare_data.py              # Unified data preparation pipeline
    ├── train.py                     # Training script
    ├── tune_hyperparameters.py      # Hyperparameter tuning
    ├── inference.py                 # Unified inference pipeline (6 modes)
    ├── app_ml_engineer.py           # Streamlit web app
    ├── model.py                     # Model architectures
    ├── data_prep/                   # Data preparation package
    │   ├── scraper.py               # Image scraping via SerpAPI
    │   ├── cleanup.py               # Duplicate/corruption removal
    │   └── splitter.py              # Train/val/test split
    ├── utils/
    │   ├── config.py                # YAML configuration loading
    │   ├── data_loaders.py          # PyTorch DataLoaders + CutMix
    │   ├── mlflow_training.py       # MLflow logging for training
    │   ├── mlflow_utils.py          # MLflow logging for inference
    │   └── system_metrics.py        # System resource monitoring
    └── explainability/
        ├── gradcam.py               # Core Grad-CAM implementation
        ├── visualizers.py           # Visualization utilities
        ├── layer_selectors.py       # Target layer selection
        ├── metrics.py               # Quality metrics (confidence drop, concentration)
        └── metadata_storage.py      # Metadata storage (JSONL)

    conf/
    ├── training_config.yaml         # Training configuration
    ├── tuning_config.yaml           # Tuning configuration
    └── inference.yaml               # Inference configuration

    checkpoints/                     # Training outputs
    └── <model>_ep<e>_bs<b>_lr<lr>/
        ├── best_model.pth
        ├── final_model.pth
        ├── training_history.json
        ├── training_diagnostics.txt
        ├── image_batches/           # Sample augmented images (inc. CutMix)
        └── results/                 # Inference outputs
            ├── evaluation_*.png
            ├── inference_metadata.jsonl
            └── heatmaps/

    checkpoints_tuning/              # Hyperparameter tuning outputs
    └── <model>/<search_method>/
        ├── trials/
        ├── tuning_results.json
        └── *.png

    mlruns/                          # MLflow tracking data
    └── <experiment_id>/
        └── <run_id>/
            ├── params/
            ├── metrics/
            ├── artifacts/
            └── tags/

    Documentation/
    ├── INFERENCE.md                 # Unified inference guide
    ├── GRADCAM.md                   # Grad-CAM implementation guide
    ├── MLFLOW_COMPLETE_GUIDE.md     # MLflow reference
    ├── SYSTEM_METRICS_GUIDE.md      # System monitoring reference
    └── runbook_notes.txt            # This file


Key Features:
    - Unified inference pipeline with YAML-driven configuration
    - Six operational modes (evaluate, explain_single, explain_batch, etc.)
    - CutMix augmentation using PyTorch built-in implementation
    - Production-ready quality metrics and suspicious prediction flagging
    - Efficient metadata storage using JSONL format
    - Interactive and CLI modes for all scripts
    - Comprehensive MLflow integration with system metrics monitoring
    - Streamlit web app for interactive inference
    - Modular code organization
    - Automatic model architecture detection
    - Progress bars with tqdm
    - Organized output structures


Pipeline Flow:
    Data Preparation → Training → Inference (6 modes)
                         ↓            ↓
                  Hyperparameter   Streamlit
                     Tuning          App

    All stages integrated with MLflow for experiment tracking.


================================================================================
SECTION 10: DOCUMENTATION
================================================================================

Complete Guides:
    - INFERENCE.md         : Unified inference pipeline (all 6 modes)
    - GRADCAM.md           : Grad-CAM implementation and quality metrics
    - MLFLOW_COMPLETE_GUIDE.md : MLflow tracking and management
    - SYSTEM_METRICS_GUIDE.md  : System resource monitoring
    - runbook_notes.txt    : This file (overview and workflows)
    - CLAUDE.md            : Project instructions for Claude Code

Each script has detailed docstrings and --help output.


================================================================================
END OF RUNBOOK
================================================================================
